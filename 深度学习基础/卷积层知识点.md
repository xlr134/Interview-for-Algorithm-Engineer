# 目录

- [1.卷积有什么特点](#user-content-1卷积有什么特点)
- [2.不同层次的卷积都提取什么类型的特征](#user-content-2不同层次的卷积都提取什么类型的特征)
- [3.卷积核大小如何选取](#user-content-3卷积核大小如何选取)
- [4.卷积感受野的相关概念](#user-content-4卷积感受野的相关概念)
- [5.网络每一层是否只能用一种尺寸的卷积核](#user-content-5网络每一层是否只能用一种尺寸的卷积核)
- [6.1*1卷积的作用](#user-content-611卷积的作用)
- [7.转置卷积的作用](#user-content-7转置卷积的作用)
- [8.空洞卷积的作用](#user-content-8空洞卷积的作用)
- [9.什么是转置卷积的棋盘效应](#user-content-9什么是转置卷积的棋盘效应)
- [10.什么是有效感受野](#user-content-10什么是有效感受野)
- [11.分组卷积的相关知识](#user-content-11分组卷积的相关知识)
- [12.什么是可变形卷积？](#12.什么是可变形卷积？)
- [13.什么是3D卷积？](#13.什么是3D卷积？)
- [14.分析一下卷积层的时间复杂度和空间复杂度](#14.分析一下卷积层的时间复杂度和空间复杂度)

<h1 id="1卷积有什么特点">1.卷积有什么特点</h1>

卷积主要有**三大特点**：

1. <font color=DeepSkyBlue>局部连接</font>。比起全连接，局部连接会大大减少网络的参数。在二维图像中，局部像素的关联性很强，设计局部连接保证了卷积网络对图像局部特征的强响应能力。

2. <font color=DeepSkyBlue>权值共享</font>。参数共享也能减少整体参数量，增强了网络训练的效率。一个卷积核的参数权重被整张图片共享，不会因为图像内位置的不同而改变卷积核内的参数权重。

3. <font color=DeepSkyBlue>下采样</font>。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。

<h1 id="2不同层次的卷积都提取什么类型的特征">2.不同层次的卷积都提取什么类型的特征</h1>

1. 浅层卷积 $\rightarrow$ 提取边缘特征

2. 中层卷积 $\rightarrow$ 提取局部特征

3. 深层卷积 $\rightarrow$ 提取全局特征

<h1 id="3卷积核大小如何选取">3.卷积核大小如何选取</h1>

最常用的是 $3\times3$ 大小的卷积核，两个 $3\times3$ 卷积核和一个 $5\times5$ 卷积核的感受野相同，但是减少了参数量和计算量，加快了模型训练。与此同时由于卷积核的增加，模型的非线性表达能力大大增强。

![](https://files.mdnice.com/user/33499/bedb2e10-0899-4577-b94c-8d83212bb8c4.png)


不过大卷积核（ $7\times7， 9\times9$ ）也有使用的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应用，大家可按需切入感兴趣的领域查看相关论文。

<h1 id="4卷积感受野的相关概念">4.卷积感受野的相关概念</h1>

目标检测和目标跟踪很多模型都会用到RPN层，anchor(目标检测)是RPN（区域提议网络）层的基础，而感受野（receptive field，RF）是anchor的基础。

**感受野的作用：**

1. $\textcolor{red}{一般来说感受野越大越好，比如分类任务中最后卷积层的感受野要大于输入图像。}$

2. 感受野足够大时，被忽略的信息就较少。

3. $\textcolor{red}{目标检测任务中设置anchor要对齐感受野，anchor太大或者偏离感受野会对性能产生一定的影响。}$

感受野计算：

![](https://files.mdnice.com/user/33499/b3f35f69-fc9a-4311-9aa7-a084baa3d9d3.png)

增大感受野的方法：

1. 使用空洞卷积$\textcolor{red}{analysis:普通卷积在进行卷积操作时，卷积核的元素是紧密相连的，相邻元素之间的间距为 1。而空洞卷积在卷积核的元素之间插入了一些空洞，通过设置一个扩张率（dilation rate）来控制空洞的大小。例如，当扩张率为 2 时，卷积核的每个元素之间会间隔一个位置，这样在进行卷积操作时，卷积核实际覆盖的区域就变大了，从而增加了感受野。)}$

2. 使用池化层

3. 增大卷积核

<h1 id="5网络每一层是否只能用一种尺寸的卷积核">5.网络每一层是否只能用一种尺寸的卷积核</h1>

常规的神经网络一般每层仅用一个尺寸的卷积核，但同一层的特征图可以分别<font color=DeepSkyBlue>使用多个不同尺寸的卷积核，以获得不同尺度的特征</font>，再把这些特征结合起来，得到的特征往往比使用单一尺寸卷积核的要好，如GoogLeNet 、Inception系列的网络，均是每层使用了多个不同的卷积核结构。如下图所示，输入的特征图在同一层分别经过 
 $1\times 1$ ， $3\times3$ 和 $5\times5$ 三种不同尺寸的卷积核，再将各自的特征图进行整合，得到的新特征可以看作不同感受野提取的特征组合，相比于单一尺寸卷积核会有更强的表达能力。

![](https://files.mdnice.com/user/33499/255d01e9-1255-427b-9d85-600860d61d13.png)

<h1 id="611卷积的作用">6.1*1卷积的作用</h1>

$1 * 1$ 卷积的作用主要有以下几点：

1. 实现特征信息的交互与整合。

2. 对特征图通道数进行升维和降维，降维时可以减少参数量。

3. $1*1$ 卷积+ 激活函数 $\rightarrow$ 增加非线性，提升网络表达能力。


![升维与降维](https://files.mdnice.com/user/33499/2d53bb9a-32e5-4876-80a9-c4f1e1907721.png)


![1 * 1卷积在GoogLeNet中的应用](https://files.mdnice.com/user/33499/92cf4bfe-dcd5-4d2e-a976-36dcee368a7f.png)

$1 * 1$ 卷积首发于NIN（Network in Network），后续也在GoogLeNet和ResNet等网络中使用。感兴趣的朋友可追踪这些论文研读细节。

<h1 id="7转置卷积的作用">7.转置卷积的作用</h1>

转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。

转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。

**转置卷积的计算公式：**

我们设卷积核尺寸为 $K\times K$ ，输入特征图为 $i \times i$ 。

（1）当 $stride = 1，padding = 0$ 时：

![](https://files.mdnice.com/user/33499/df26b9a5-8875-4ccc-96ef-bfb523942e9e.gif)

输入特征图在进行转置卷积操作时相当于进行了 $padding = K - 1$ 的填充，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $i + (K - 1)$

（2）当 $stride > 1，padding = 0$ 时：

![](https://files.mdnice.com/user/33499/da594d05-2e5f-46c9-b2c3-81bd31f8961f.gif)

输入特征图在进行转置卷积操作时相当于进行了 $padding = K - 1$ 的填充，相邻元素间的空洞大小为 $stride - 1$ ，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $stride * (i - 1) + K$

<h1 id="8空洞卷积的作用">8.空洞卷积的作用</h1>

$\textcolor{red}{空洞卷积的作用是在不进行池化操作损失信息的情况下，增大感受野，让每个卷积输出都包含较大范围的信息}$

空洞卷积有一个参数可以设置dilation rate，其在卷积核中填充dilation rate个0，因此，当设置不同dilation rate时，感受野就会不一样，也获取了多尺度信息。

![](https://files.mdnice.com/user/33499/03827ca6-6abb-4565-a924-91983bc5611d.png)

(a) 图对应 $3\times3$ 的1-dilated conv，和普通的卷积操作一样。(b)图对应 $3\times3$ 的2-dilated conv，实际的卷积kernel size还是 $3\times3$ ，但是空洞为 $1$ ，也就是对于一个 $7\times7$ 的图像patch，只有$9$个红色的点和 $3\times3$ 的kernel发生卷积操作，其余的点的权重为 $0$ 。(c)图是4-dilated conv操作。

<h1 id="9什么是转置卷积的棋盘效应">9.什么是转置卷积的棋盘效应</h1>

![](https://files.mdnice.com/user/33499/34a6b1e9-a4dd-4bc6-8979-492773be51f7.png)

造成棋盘效应的原因是转置卷积的不均匀重叠（uneven overlap）。这种重叠会造成图像中某个部位的颜色比其他部位更深。

在下图展示了棋盘效应的形成过程，深色部分代表了不均匀重叠：

![棋盘效应](https://files.mdnice.com/user/33499/c4a37f1c-8843-4425-ba64-01b3a9db2499.png)

接下来我们将卷积步长改为2，可以看到输出图像上的所有像素从输入图像中接收到同样多的信息，它们都从输入图像中接收到一个像素的信息，这样就不存在转置卷带来的重叠区域。

![](https://files.mdnice.com/user/33499/797f9ae7-6b34-438a-b67e-355066183421.png)

我们也可以直接进行插值Resize操作，然后再进行卷积操作来消除棋盘效应。这种方式在超分辨率重建场景中比较常见。例如使用双线性插值和近邻插值等方法来进行上采样。

<h1 id="10什么是有效感受野">10.什么是有效感受野</h1>

感受野的相关知识在上面的第四节中中介绍过。

我们接着再看看有效感受野(effective receptive field, ERF)的相关知识。

一般而言，feature map上有效感受野要小于实际感受野。其有效性，以中心点为基准，类似高斯分布向边缘递减。

总的来说，感受野主要描述feature map中的最大信息量，有效感受野则主要描述信息的有效性。

<h1 id="11分组卷积的相关知识">11.分组卷积的相关知识</h1>

分组卷积（Group Convolution）最早出现在AlexNet网络中，分组卷积被用来切分网络，使其能在多个GPU上并行运行。

![分组卷积和普通卷积的区别](https://img-blog.csdnimg.cn/20201104211649626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvY2t5NjY4OA==,size_16,color_FFFFFF,t_70#pic_center)

普通卷积进行运算的时候，如果输入feature map尺寸是 $C\times H \times W$ ，卷积核有N个，那么输出的feature map与卷积核的数量相同也是N个，每个卷积核的尺寸为 $C\times K \times K$ ，N个卷积核的总参数量为 $N \times C \times K \times K$ 。

分组卷积的主要对输入的feature map进行分组，然后每组分别进行卷积。如果输入feature map尺寸是 $C\times H \times W$ ，输出feature map的数量为 $N$ 个，如果我们设定要分成G个group，则每组的输入feature map数量为 $\frac{C}{G}$ ，则每组的输出feature map数量为 $\frac{N}{G}$ ，每个卷积核的尺寸为 $\frac{C}{G} \times K \times K$ ，卷积核的总数仍为N个，每组的卷积核数量为 $\frac{N}{G}$ ，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为 $N \times \frac{C}{G} \times K \times K$ ，<font color=DeepSkyBlue>易得总的参数量减少为原来的 $\frac{1}{G}$ </font>。

**分组卷积的作用:**

1. 分组卷积可以减少参数量。

2. 分组卷积可以看成是稀疏操作，有时可以在较少参数量的情况下获得更好的效果（相当于正则化操作）。

3. 当分组数量等于输入feature map通道数量，输出feature map数量也等于输入feature map数量时，分组卷积就成了Depthwise卷积，可以使参数量进一步缩减。


<h1 id="12.什么是可变形卷积？">12.什么是可变形卷积？</h1>

论文链接：https://openaccess.thecvf.com/content_ICCV_2017/papers/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.pdf

可变形卷积（Deformable Convolution）是一种改进传统卷积神经网络（CNN）能力的新型卷积方法，旨在解决CNN在建模几何变换方面的固有限制。传统的卷积操作在固定的几何结构上进行采样，这使得它们在面对不同对象的尺度、姿态、视角和部分变形时表现不佳。

### 可变形卷积的工作原理

可变形卷积通过在标准卷积的采样网格位置上添加2D偏移量，使得卷积核能够在自由形式的采样位置上进行操作。这些偏移量通过前面的特征图生成，并通过额外的卷积层进行学习，从而使得变形是基于输入特征局部和自适应的。

如下图所示：

1. **输入特征图（Input Feature Map）**：传统卷积在输入特征图上以固定网格采样。
2. **卷积核（Conv）**：在标准卷积中，卷积核在特征图上滑动，生成输出特征图。
3. **偏移场（Offset Field）**：在可变形卷积中，偏移量由前一层特征图生成，并与卷积核一起应用于输入特征图。
4. **输出特征图（Output Feature Map）**：最终生成的输出特征图具有更强的适应性，能够更好地捕捉图像中的几何变换。

![image-20240524120850631](./imgs/可变形卷积示意图.png)

![image-20240524120918705](./imgs/可变形卷积感受野.png)

标准卷积(a)和可变形卷积(b)中固定感受野和自适应感受野的示意图如上图所示。


<h1 id="13.什么是3D卷积？">13.什么是3D卷积？</h1>

3D卷积（3D Convolution）是一种用于处理三维数据的卷积运算。与我们常见的2D卷积不同，3D卷积不仅在图像的宽和高两个空间维度上滑动，还在第三个维度上滑动，通常是时间或深度维度。因此，3D卷积通常用于处理如视频、医学影像（如MRI或CT扫描）或任何具有时间、深度信息的三维数据。

### 3D卷积的基本原理

要理解3D卷积的原理，我们可以从2D卷积入手。假设我们有一个2D卷积核（kernel），它在图像上滑动，生成一个特征图（Feature Map）。这个过程可以类比为在图片上用滤镜移动，提取特定的特征，例如边缘、纹理等。

现在，想象一下，我们不仅在图像的宽度和高度方向滑动卷积核，还在第三个维度上滑动，这个维度可以是时间（如视频中的帧）、深度（如CT扫描中的层）、或者颜色通道的组合。

### 3D卷积的过程

1. **输入数据**：
   - 3D卷积的输入通常是一个三维特征，例如一个视频可以表示为 `(帧数, 高度, 宽度)`，或一个立体图像（如3D医疗图像）表示为 `(深度, 高度, 宽度)`。

2. **卷积核（Filter/Kernels）**：
   - 3D卷积核是一个三维的小块。例如，一个 $3\times3\times3$ 的卷积核在三个维度上分别有3个单元。
   - 卷积核在输入数据的三个维度上滑动，在每一个位置进行点积运算（每个元素相乘再整体相加），生成一个输出值。

3. **输出特征图**：
   - 输出特征也是一个三维特征图，它记录了卷积核在输入数据上每个位置滑动时的运算结果。输出的深度由输入数据的深度和卷积核的深度决定。

### 类比：用模具制作巧克力

想象一下，我们在制作巧克力，模具就是我们的卷积核。我们有一个厚厚的巧克力板（3D输入数据），你要在这块板上用模具压出不同形状的巧克力（输出特征图）。

- **2D卷积**：就像在一块平面的巧克力上用模具压出形状，我们只能在平面上移动模具（左右、上下），每次我们都会得到一个平面巧克力的形状。
  
- **3D卷积**：现在我们不仅可以在平面上移动模具，还可以把模具往巧克力的厚度方向压进去。这样，我们每次压出的是一个立体的巧克力块。这块立体巧克力反映了我们在厚度方向（如时间、深度）上也进行了一次处理。

### 3D卷积的应用场景

1. **AI视频领域**：
   - 在AI视频领域中，3D卷积的作用非常明显，Sora、CogVideoX等主流AI视频大模型中都用到了3D卷积。**视频数据通常是多个连续的帧序列，通过在时间维度上进行卷积，AI视频模型可以捕捉到动作的动态特征**。

2. **医学影像**：
   - 在医学影像中，如MRI或CT扫描，3D卷积可以在扫描的多个切片上滑动，以捕捉人体内部结构的三维特征，这对于诊断和分析非常重要。

3. **人体姿态估计**：
   - 3D卷积可以用于从多帧图像中提取人体的姿态，帮助模型理解姿态变化和动作。

### 3D卷积的计算公式

3D卷积的输入通常是一个四维张量，形状为 `(D, H, W, C)`，其中：
- `D` 是深度（例如视频中的帧数或医学图像中的切片数）。
- `H` 是高度。
- `W` 是宽度。
- `C` 是通道数（例如，RGB图像的通道数为3）。

假设我们有一个输入张量 `X`，其形状为 `(D_in, H_in, W_in, C_in)`，其中 `C_in` 是输入的通道数。我们还有一个卷积核 `W`，其形状为 `(D_k, H_k, W_k, C_in, C_out)`，其中 `C_out` 是输出的通道数。

#### 计算过程步骤如下：

1. **滑动卷积核**：
   - 卷积核 `W` 在输入张量 `X` 上按步长（stride）滑动。在每个位置，卷积核的每个元素与对应位置的输入数据相乘，并将这些乘积相加得到一个输出值。

2. **点积运算**：
   - 在每个滑动位置，对输入张量与卷积核进行点积运算。具体地，对于某个位置 `(i, j, k)` 的输出值，计算公式为：
    
     $$Y(i, j, k, m) = \sum_{d=0}^{D_k-1} \sum_{h=0}^{H_k-1} \sum_{w=0}^{W_k-1} \sum_{c=0}^{C_in-1} X(i+d, j+h, k+w, c) \cdot W(d, h, w, c, m)$$
     
     其中：
     - `Y(i, j, k, m)` 是输出张量在位置 `(i, j, k)` 和通道 `m` 上的值。
     - `X(i+d, j+h, k+w, c)` 是输入张量 `X` 在位置 `(i+d, j+h, k+w)` 和通道 `c` 上的值。
     - `W(d, h, w, c, m)` 是卷积核 `W` 在位置 `(d, h, w)` 和输入通道 `c`、输出通道 `m` 上的权重。

3. **应用偏置**：
   - 对于每个输出通道，可以添加一个偏置 `b[m]`，使得最终的输出为：
   
     $$Y(i, j, k, m) = Y(i, j, k, m) + b[m]$$

4. **输出张量**：
   - 经过上述操作后，得到的输出张量 `Y` 的形状为 `(D_out, H_out, W_out, C_out)`，其中 `D_out`、`H_out` 和 `W_out` 分别是深度、高度和宽度方向上的输出尺寸。

#### 举例说明:

假设我们有一个输入张量 `X`，其形状为 `(4, 4, 4, 1)`，即深度、高度、宽度均为4，且有一个输入通道。我们有一个卷积核 `W`，其形状为 `(2, 2, 2, 1, 1)`，即大小为2x2x2的立方体卷积核，有1个输入通道和1个输出通道。步长为1，无填充（padding）。

1. **卷积核在输入张量上滑动**：
   - 卷积核首先覆盖输入张量的前2x2x2部分，与这部分的输入数据进行点积运算，得到一个输出值。
   - 然后，卷积核滑动到下一个位置，重复上述过程，直到覆盖所有可能的位置。

2. **计算每个位置的输出**：
   - 例如，对于第一个位置（顶点）上的输出值，计算公式为：

     $$Y(0, 0, 0, 0) = X(0, 0, 0, 0) \cdot W(0, 0, 0, 0, 0) + X(0, 0, 1, 0) \cdot W(0, 0, 1, 0, 0) + \ldots + X(1, 1, 1, 0) \cdot W(1, 1, 1, 0, 0)$$

3. **得到最终输出**：
   - 经过所有位置的计算后，得到的输出张量 `Y` 的形状为 `(3, 3, 3, 1)`。

### 3D卷积的优缺点

**优点**：
- 能够捕捉到输入数据的三维特征，尤其适合处理视频数据和3D图像数据。
- 能够捕捉时间维度（或深度维度）的变化，适用于动态特征提取。

**缺点**：
- 计算复杂度更高，训练时间更长。
- 需要更多的内存和计算资源。


<h1 id="14.分析一下卷积层的时间复杂度和空间复杂度">14.分析一下卷积层的时间复杂度和空间复杂度</h1>

### 一、复杂度定义与计算公式

**1. 时间复杂度（计算量）**  
时间复杂度衡量卷积层的计算资源消耗，通常用**浮点运算次数（FLOPs）**表示。  
**公式**：  
$$\text{FLOPs} = 2 \times K^2 \times C_{\text{in}} \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}$$

其中：  
- $K$ ：卷积核尺寸（假设为正方形核）  
- $C_{\text{in}}$ ：输入通道数  
- $C_{\text{out}}$ ：输出通道数  
- $H_{\text{out}}, W_{\text{out}}$ ：输出特征图的高度和宽度  
**注**：乘法和加法各计1次运算，因此系数为2。

**2. 空间复杂度（参数量与内存占用）**  
空间复杂度包含两部分：  
- **参数量**：模型存储的权重参数数量  
$$\text{Params} = K^2 \times C_{\text{in}} \times C_{\text{out}}$$

- **激活内存**：前向传播中特征图的存储需求  
  $$\text{Memory} = C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times \text{bytes-per-element}$$

  （通常每个元素为4字节的float32类型）

#### 二、通俗案例解析  
**案例：手机端图像分类模型**  
假设输入为RGB图像（3×224×224），使用一个卷积层：  
- 卷积核：3×3，输出通道64，步长1，padding=1  
- 输出尺寸：64×224×224  

**复杂度计算**：  
- **时间复杂度**：  
  $$2 \times 3^2 \times 3 \times 64 \times 224 \times 224 = 1.73 \times 10^9 \, \text{FLOPs}$$

- **参数量**：  

  $$3^2 \times 3 \times 64 = 1,728 \, \text{参数}$$
  
- **激活内存**：  
  
  $$64 \times 224 \times 224 \times 4 \, \text{Bytes} = 12.58 \, \text{MB}$$
  

**优化对比**：  
若改用**深度可分离卷积**（Depthwise Separable Convolution）：  
- 参数量降至 $3^2 \times 3 + 3 \times 64 = 27 + 192 = 219$
- FLOPs降至 $3^2 \times 3 \times 224^2 + 3 \times 64 \times 224^2 = 0.13 \times 10^9 \, \text{FLOPs}$
计算量减少约**13倍**，适用于移动端部署。

